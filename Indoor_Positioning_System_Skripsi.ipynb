{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxKeyaxx/Multiple-Floor-Indoor-Positioning-System-using-GraphSage-and-Transfer-Learning/blob/main/Indoor_Positioning_System_Skripsi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ff_T-gGK3Mn",
        "outputId": "af0a7da8-6adc-4faf-a6a5-389f123d2425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "test_file_path = '/content/drive/My Drive/UTSIndoorLoc/UTS_test.csv'\n",
        "train_file_path = '/content/drive/My Drive/UTSIndoorLoc/UTS_training.csv'\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "df_train = pd.read_csv(train_file_path)\n",
        "print(df_train.head())  # Preview the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNJuNYOnLO1y",
        "outputId": "67c6abf2-206a-454a-ea7e-e478ec8a5071"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
            "0     100     100     100     100     100     100     100     100     100   \n",
            "1     100     100     100     100     100     100     100     100     100   \n",
            "2     100     100     100     100     100     100     100     100     100   \n",
            "3     100     100     100     100     100     100     100     100     100   \n",
            "4     100     100     100     100     100     100     100     100     100   \n",
            "\n",
            "   WAP010  ...  WAP587  WAP588  WAP589    Pos_x    Pos_y  Floor_ID  \\\n",
            "0     100  ...     100     100     100  40.7871  31.0272         8   \n",
            "1     100  ...     100     100     100  40.7871  31.0272         8   \n",
            "2     100  ...     100     100     100  40.7871  31.0272         8   \n",
            "3     100  ...     100     100     100  39.4464  30.7985         8   \n",
            "4     100  ...     100     100     100  39.4464  30.7985         8   \n",
            "\n",
            "   Building_ID  User_ID  Phone_type             Time  \n",
            "0       New IT     root      unknow  6/03/2019 22:29  \n",
            "1       New IT     root      unknow  6/03/2019 22:29  \n",
            "2       New IT     root      unknow  6/03/2019 22:29  \n",
            "3       New IT     root      unknow  6/03/2019 22:30  \n",
            "4       New IT     root      unknow  6/03/2019 22:30  \n",
            "\n",
            "[5 rows x 596 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming data is in a Pandas DataFrame\n",
        "floor_datasets = {}\n",
        "for floor_id in df_train[\"Floor_ID\"].unique():\n",
        "    floor_datasets[floor_id] = df_train[df_train[\"Floor_ID\"] == floor_id]"
      ],
      "metadata": {
        "id": "fsqbzh53gPZn"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    # Replace 100 with 0 (no detection)\n",
        "    sensor_cols = [col for col in data.columns if col.startswith('WAP')]\n",
        "    # data[sensor_cols] = data[sensor_cols].replace(100, 0)\n",
        "    return data, sensor_cols\n",
        "\n",
        "df_train, sensor_cols = preprocess_data(df_train)\n",
        "# df_train = df_train[df_train['Floor_ID'] == 12]\n",
        "\n",
        "# Load and preprocess the new dataset\n",
        "df_test, sensor_cols = preprocess_data(df_test)\n",
        "# max_floor = 12\n",
        "# df_test = df_test[df_test['Floor_ID'] == max_floor]"
      ],
      "metadata": {
        "id": "wX4-SUESVhmv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "INCjUzZ-pt54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def data_split(data):\n",
        "    x = data[sensor_cols].values\n",
        "    y = data[['Pos_x', 'Pos_y', 'Floor_ID']].values\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = data_split(df_train)\n",
        "x_test, y_test = data_split(df_test)\n",
        "\n",
        "# Initialize the SVR model\n",
        "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.2)\n",
        "\n",
        "# Wrap SVR in MultiOutputRegressor\n",
        "multi_output_svr = MultiOutputRegressor(svr_model)\n",
        "\n",
        "# Train the model\n",
        "multi_output_svr.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = multi_output_svr.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "# mae = mean_absolute_error(y_test, y_pred)\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# rmse = np.sqrt(mse)\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# print(\"Evaluation Metrics:\")\n",
        "# print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "# print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "# print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "# print(f\"R2 Score: {r2:.4f}\")\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')  # Evaluate for each output\n",
        "\n",
        "print(\"Mean Absolute Error for each output:\")\n",
        "print(mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK7YCCJ7pvcv",
        "outputId": "c10f6726-9b1b-4b40-88f7-bac4124e49a5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error for each output:\n",
            "[6.7166584  2.52376129 0.3891885 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAT"
      ],
      "metadata": {
        "id": "AL6PZ-c0pv3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create graph data for each floor\n",
        "# def create_graphs_by_floor(data, sensor_cols):\n",
        "#     graphs = {}\n",
        "#     for floor_id in data['Floor_ID'].unique():\n",
        "#         floor_data = data[data['Floor_ID'] == floor_id]\n",
        "#         x = torch.tensor(floor_data[sensor_cols].values, dtype=torch.float)\n",
        "#         y = torch.tensor(floor_data[['Pos_x', 'Pos_y']].values, dtype=torch.float)\n",
        "\n",
        "#         # Create dummy edges (fully connected graph for simplicity)\n",
        "#         edge_index = torch.combinations(torch.arange(x.size(0)), r=2).T\n",
        "#         edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
        "\n",
        "#         graphs[floor_id] = Data(x=x, edge_index=edge_index, y=y)\n",
        "#     return graphs\n",
        "\n",
        "# graphs_by_floor = create_graphs_by_floor(df_train, sensor_cols)\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import numpy as np\n",
        "\n",
        "def create_knn_graphs_by_floor(data, sensor_cols, k=5):\n",
        "    graphs = {}\n",
        "    for floor_id in data['Floor_ID'].unique():\n",
        "        floor_data = data[data['Floor_ID'] == floor_id]\n",
        "        x = torch.tensor(floor_data[sensor_cols].values, dtype=torch.float)\n",
        "        y = torch.tensor(floor_data[['Pos_x', 'Pos_y', 'Floor_ID']].values, dtype=torch.float)\n",
        "\n",
        "        # Compute pairwise distances\n",
        "        distances = euclidean_distances(floor_data[sensor_cols])\n",
        "        edge_index = []\n",
        "\n",
        "        # For each node, find k nearest neighbors\n",
        "        for i in range(distances.shape[0]):\n",
        "            neighbors = np.argsort(distances[i])[:k]  # Get indices of k nearest neighbors\n",
        "            for j in neighbors:\n",
        "                edge_index.append([i, j])\n",
        "\n",
        "        # Convert edge list to PyTorch Geometric format\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).T\n",
        "\n",
        "        graphs[floor_id] = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return graphs\n",
        "\n",
        "graphs_by_floor = create_knn_graphs_by_floor(df_train, sensor_cols, k=5)\n"
      ],
      "metadata": {
        "id": "mF61bi4MZhis"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class GraphSAGE(torch.nn.Module):\n",
        "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "#         super(GraphSAGE, self).__init__()\n",
        "#         self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "#         self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         x = self.conv1(x, edge_index).relu()\n",
        "#         x = self.conv2(x, edge_index)\n",
        "#         return x\n",
        "\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, concat=True)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Initialize the GAT model\n",
        "model = GAT(\n",
        "    in_channels=len(sensor_cols),\n",
        "    hidden_channels=64,\n",
        "    out_channels=3,\n",
        "    heads=3  # Multi-head attention for the first layer\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "MlH20YCraOUz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize and train the model for one floor\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = GraphSAGE(in_channels=len(sensor_cols), hidden_channels=64, out_channels=2).to(device)\n",
        "\n",
        "# # Train on a single floor and fine-tune on another\n",
        "# def train_model(graph_data, model, epochs=100, lr=0.01):\n",
        "#     graph_data = graph_data.to(device)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "#     criterion = torch.nn.MSELoss()\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         pred = model(graph_data.x, graph_data.edge_index)\n",
        "#         loss = criterion(pred, graph_data.y)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         if (epoch + 1) % 20 == 0:\n",
        "#             print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "#     return model\n",
        "\n",
        "def train_model(graph_data, model, epochs=200, lr=0.01):\n",
        "    graph_data = graph_data.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(graph_data.x, graph_data.edge_index)\n",
        "        loss = criterion(pred, graph_data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "    return model"
      ],
      "metadata": {
        "id": "YWNXlydBhBzH"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_floor = 0\n",
        "for floor_id in sorted(graphs_by_floor.keys()):\n",
        "    print(f\"Training/Fine-tuning on Floor {floor_id}...\")\n",
        "    model = train_model(graphs_by_floor[floor_id], model)\n",
        "    max_floor = floor_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKdksSzdnqf3",
        "outputId": "b69e05bb-42d7-439c-b899-045089591b61"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Fine-tuning on Floor -3...\n",
            "Epoch 20, Loss: 4391.08642578125\n",
            "Epoch 40, Loss: 558.4625854492188\n",
            "Epoch 60, Loss: 25.355018615722656\n",
            "Epoch 80, Loss: 21.469118118286133\n",
            "Epoch 100, Loss: 12.596022605895996\n",
            "Epoch 120, Loss: 10.984979629516602\n",
            "Epoch 140, Loss: 9.953471183776855\n",
            "Epoch 160, Loss: 9.176854133605957\n",
            "Epoch 180, Loss: 8.527734756469727\n",
            "Epoch 200, Loss: 7.931508541107178\n",
            "Training/Fine-tuning on Floor -2...\n",
            "Epoch 20, Loss: 67.92457580566406\n",
            "Epoch 40, Loss: 78.45779418945312\n",
            "Epoch 60, Loss: 31.839820861816406\n",
            "Epoch 80, Loss: 12.57603931427002\n",
            "Epoch 100, Loss: 9.01495361328125\n",
            "Epoch 120, Loss: 7.941660404205322\n",
            "Epoch 140, Loss: 7.298122882843018\n",
            "Epoch 160, Loss: 6.769341468811035\n",
            "Epoch 180, Loss: 6.304450511932373\n",
            "Epoch 200, Loss: 5.884345531463623\n",
            "Training/Fine-tuning on Floor -1...\n",
            "Epoch 20, Loss: 187.65869140625\n",
            "Epoch 40, Loss: 86.3884048461914\n",
            "Epoch 60, Loss: 26.526016235351562\n",
            "Epoch 80, Loss: 15.775426864624023\n",
            "Epoch 100, Loss: 13.002857208251953\n",
            "Epoch 120, Loss: 11.4664888381958\n",
            "Epoch 140, Loss: 10.382078170776367\n",
            "Epoch 160, Loss: 9.565450668334961\n",
            "Epoch 180, Loss: 8.919288635253906\n",
            "Epoch 200, Loss: 8.399201393127441\n",
            "Training/Fine-tuning on Floor 0...\n",
            "Epoch 20, Loss: 276.1041259765625\n",
            "Epoch 40, Loss: 91.62127685546875\n",
            "Epoch 60, Loss: 44.204776763916016\n",
            "Epoch 80, Loss: 31.198719024658203\n",
            "Epoch 100, Loss: 25.188610076904297\n",
            "Epoch 120, Loss: 20.6955509185791\n",
            "Epoch 140, Loss: 17.17367935180664\n",
            "Epoch 160, Loss: 14.718435287475586\n",
            "Epoch 180, Loss: 13.048212051391602\n",
            "Epoch 200, Loss: 11.843804359436035\n",
            "Training/Fine-tuning on Floor 1...\n",
            "Epoch 20, Loss: 84.34083557128906\n",
            "Epoch 40, Loss: 28.80538558959961\n",
            "Epoch 60, Loss: 20.446006774902344\n",
            "Epoch 80, Loss: 17.19139289855957\n",
            "Epoch 100, Loss: 15.589662551879883\n",
            "Epoch 120, Loss: 14.4398193359375\n",
            "Epoch 140, Loss: 13.527204513549805\n",
            "Epoch 160, Loss: 12.745780944824219\n",
            "Epoch 180, Loss: 12.066328048706055\n",
            "Epoch 200, Loss: 11.464316368103027\n",
            "Training/Fine-tuning on Floor 2...\n",
            "Epoch 20, Loss: 65.89472198486328\n",
            "Epoch 40, Loss: 22.411909103393555\n",
            "Epoch 60, Loss: 14.35159969329834\n",
            "Epoch 80, Loss: 10.014538764953613\n",
            "Epoch 100, Loss: 7.643505573272705\n",
            "Epoch 120, Loss: 6.096668243408203\n",
            "Epoch 140, Loss: 5.025637149810791\n",
            "Epoch 160, Loss: 4.265164375305176\n",
            "Epoch 180, Loss: 3.702225923538208\n",
            "Epoch 200, Loss: 3.275771141052246\n",
            "Training/Fine-tuning on Floor 3...\n",
            "Epoch 20, Loss: 55.47187805175781\n",
            "Epoch 40, Loss: 25.28369140625\n",
            "Epoch 60, Loss: 15.530963897705078\n",
            "Epoch 80, Loss: 9.195728302001953\n",
            "Epoch 100, Loss: 6.3835248947143555\n",
            "Epoch 120, Loss: 9.840702056884766\n",
            "Epoch 140, Loss: 5.191882133483887\n",
            "Epoch 160, Loss: 3.9767301082611084\n",
            "Epoch 180, Loss: 3.359729051589966\n",
            "Epoch 200, Loss: 2.9298031330108643\n",
            "Training/Fine-tuning on Floor 4...\n",
            "Epoch 20, Loss: 55.562110900878906\n",
            "Epoch 40, Loss: 41.99197769165039\n",
            "Epoch 60, Loss: 37.16122055053711\n",
            "Epoch 80, Loss: 34.51519775390625\n",
            "Epoch 100, Loss: 33.08417510986328\n",
            "Epoch 120, Loss: 32.69176483154297\n",
            "Epoch 140, Loss: 30.07076072692871\n",
            "Epoch 160, Loss: 28.714439392089844\n",
            "Epoch 180, Loss: 27.408695220947266\n",
            "Epoch 200, Loss: 26.54752540588379\n",
            "Training/Fine-tuning on Floor 5...\n",
            "Epoch 20, Loss: 73.58324432373047\n",
            "Epoch 40, Loss: 39.00297927856445\n",
            "Epoch 60, Loss: 31.325193405151367\n",
            "Epoch 80, Loss: 28.212722778320312\n",
            "Epoch 100, Loss: 26.031112670898438\n",
            "Epoch 120, Loss: 24.453998565673828\n",
            "Epoch 140, Loss: 23.34585189819336\n",
            "Epoch 160, Loss: 22.528921127319336\n",
            "Epoch 180, Loss: 21.83184242248535\n",
            "Epoch 200, Loss: 21.263011932373047\n",
            "Training/Fine-tuning on Floor 6...\n",
            "Epoch 20, Loss: 88.1876220703125\n",
            "Epoch 40, Loss: 58.130069732666016\n",
            "Epoch 60, Loss: 49.2626838684082\n",
            "Epoch 80, Loss: 44.00044631958008\n",
            "Epoch 100, Loss: 40.71909713745117\n",
            "Epoch 120, Loss: 38.16450881958008\n",
            "Epoch 140, Loss: 36.0971794128418\n",
            "Epoch 160, Loss: 34.342159271240234\n",
            "Epoch 180, Loss: 32.84344482421875\n",
            "Epoch 200, Loss: 31.566471099853516\n",
            "Training/Fine-tuning on Floor 7...\n",
            "Epoch 20, Loss: 94.33758544921875\n",
            "Epoch 40, Loss: 72.28640747070312\n",
            "Epoch 60, Loss: 66.22053527832031\n",
            "Epoch 80, Loss: 61.5804443359375\n",
            "Epoch 100, Loss: 58.39940643310547\n",
            "Epoch 120, Loss: 55.80588150024414\n",
            "Epoch 140, Loss: 53.49285888671875\n",
            "Epoch 160, Loss: 50.58392333984375\n",
            "Epoch 180, Loss: 48.51204299926758\n",
            "Epoch 200, Loss: 46.409576416015625\n",
            "Training/Fine-tuning on Floor 8...\n",
            "Epoch 20, Loss: 156.8687286376953\n",
            "Epoch 40, Loss: 107.0218734741211\n",
            "Epoch 60, Loss: 90.45240020751953\n",
            "Epoch 80, Loss: 82.3359146118164\n",
            "Epoch 100, Loss: 75.76554107666016\n",
            "Epoch 120, Loss: 70.18238830566406\n",
            "Epoch 140, Loss: 65.29352569580078\n",
            "Epoch 160, Loss: 61.503177642822266\n",
            "Epoch 180, Loss: 58.10715866088867\n",
            "Epoch 200, Loss: 55.49320602416992\n",
            "Training/Fine-tuning on Floor 9...\n",
            "Epoch 20, Loss: 115.26988220214844\n",
            "Epoch 40, Loss: 47.70018005371094\n",
            "Epoch 60, Loss: 39.41686248779297\n",
            "Epoch 80, Loss: 35.974666595458984\n",
            "Epoch 100, Loss: 33.7594108581543\n",
            "Epoch 120, Loss: 32.18446731567383\n",
            "Epoch 140, Loss: 30.848098754882812\n",
            "Epoch 160, Loss: 29.731060028076172\n",
            "Epoch 180, Loss: 28.773696899414062\n",
            "Epoch 200, Loss: 27.937387466430664\n",
            "Training/Fine-tuning on Floor 10...\n",
            "Epoch 20, Loss: 51.60682678222656\n",
            "Epoch 40, Loss: 29.61498260498047\n",
            "Epoch 60, Loss: 26.418560028076172\n",
            "Epoch 80, Loss: 23.73936653137207\n",
            "Epoch 100, Loss: 21.948715209960938\n",
            "Epoch 120, Loss: 20.593137741088867\n",
            "Epoch 140, Loss: 19.560226440429688\n",
            "Epoch 160, Loss: 18.60451316833496\n",
            "Epoch 180, Loss: 17.866985321044922\n",
            "Epoch 200, Loss: 17.240108489990234\n",
            "Training/Fine-tuning on Floor 11...\n",
            "Epoch 20, Loss: 126.33318328857422\n",
            "Epoch 40, Loss: 42.26692581176758\n",
            "Epoch 60, Loss: 32.99861145019531\n",
            "Epoch 80, Loss: 28.20588493347168\n",
            "Epoch 100, Loss: 25.49440574645996\n",
            "Epoch 120, Loss: 23.445144653320312\n",
            "Epoch 140, Loss: 21.869853973388672\n",
            "Epoch 160, Loss: 20.638864517211914\n",
            "Epoch 180, Loss: 19.67306900024414\n",
            "Epoch 200, Loss: 18.907176971435547\n",
            "Training/Fine-tuning on Floor 12...\n",
            "Epoch 20, Loss: 95.88219451904297\n",
            "Epoch 40, Loss: 37.43312454223633\n",
            "Epoch 60, Loss: 30.57439613342285\n",
            "Epoch 80, Loss: 27.515085220336914\n",
            "Epoch 100, Loss: 25.263900756835938\n",
            "Epoch 120, Loss: 23.551088333129883\n",
            "Epoch 140, Loss: 22.198841094970703\n",
            "Epoch 160, Loss: 21.125333786010742\n",
            "Epoch 180, Loss: 20.280040740966797\n",
            "Epoch 200, Loss: 19.617387771606445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# # Evaluate the global model on a new dataset\n",
        "# def evaluate_model(model, test_graph):\n",
        "#     model.eval()\n",
        "#     test_graph = test_graph.to(device)\n",
        "#     with torch.no_grad():\n",
        "#         pred = model(test_graph.x, test_graph.edge_index)\n",
        "#         true = test_graph.y\n",
        "\n",
        "#         # Calculate Mean Absolute Error (MAE)\n",
        "#         mae = torch.mean(torch.abs(pred - true))\n",
        "\n",
        "#         # Calculate Root Mean Square Error (RMSE)\n",
        "#         rmse = torch.sqrt(torch.mean((pred - true) ** 2))\n",
        "\n",
        "#         r2 = r2_score(true.cpu().numpy(), pred.cpu().numpy())\n",
        "\n",
        "#     print(\"Evaluation Results:\")\n",
        "#     print(f\"MAE: {mae.item():.4f}\")\n",
        "#     print(f\"RMSE: {rmse.item():.4f}\")\n",
        "#     print(f\"R2 Score: {r2:.4f}\")\n",
        "#     return mae.item(), rmse.item()\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Evaluate the global model on a new dataset\n",
        "def evaluate_model(model, test_graph):\n",
        "    model.eval()\n",
        "    test_graph = test_graph.to(device)\n",
        "    with torch.no_grad():\n",
        "        pred = model(test_graph.x, test_graph.edge_index)  # Predict positions\n",
        "        predx = pred[:, :1]\n",
        "        predy = pred[:, 1:2]\n",
        "        true = test_graph.y  # True positions\n",
        "        truex = true[:, :1]\n",
        "        truey = true[:, 1:2]\n",
        "\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE)\n",
        "        maex = torch.mean(torch.abs(predx - truex))\n",
        "\n",
        "        # Calculate Root Mean Square Error (RMSE)\n",
        "        rmsex = torch.sqrt(torch.mean((predx - truex) ** 2))\n",
        "\n",
        "        # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "        mapex = torch.mean(torch.abs((predx - truex) / truex)) * 100\n",
        "\n",
        "        # Calculate R2 Score\n",
        "        r2x = r2_score(truex.cpu().numpy(), predx.cpu().numpy())\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE)\n",
        "        maey = torch.mean(torch.abs(predy - truey))\n",
        "\n",
        "        # Calculate Root Mean Square Error (RMSE)\n",
        "        rmsey = torch.sqrt(torch.mean((predy - truey) ** 2))\n",
        "\n",
        "        # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "        mapey = torch.mean(torch.abs((predy - truey) / truey)) * 100\n",
        "\n",
        "        # Calculate R2 Score\n",
        "        r2y = r2_score(truey.cpu().numpy(), predy.cpu().numpy())\n",
        "\n",
        "    print(\"Evaluation Results X:\")\n",
        "    print(f\"MAE: {maex.item():.4f}\") # Probably main evaluator\n",
        "    print(f\"RMSE: {rmsex.item():.4f}\")\n",
        "    print(f\"MAPE: {mapex.item():.4f}%\")\n",
        "    # print(f\"R2 Score: {r2x:.4f}\") # not effective because data not in a normal distribution\n",
        "\n",
        "    print(\"Evaluation Results Y:\")\n",
        "    print(f\"MAE: {maey.item():.4f}\") # Probably main evaluator\n",
        "    print(f\"RMSE: {rmsey.item():.4f}\")\n",
        "    print(f\"MAPE: {mapey.item():.4f}%\")\n",
        "    # print(f\"R2 Score: {r2y:.4f}\") # not effective because data not in a normal distribution\n",
        "\n",
        "\n",
        "# # Create a unified graph without floor consideration\n",
        "# def create_unified_graph(data, sensor_cols):\n",
        "#     # Features: RSSI values\n",
        "#     x = torch.tensor(data[sensor_cols].values, dtype=torch.float)\n",
        "#     # Labels: Coordinates (Pos_x, Pos_y)\n",
        "#     y = torch.tensor(data[['Pos_x', 'Pos_y']].values, dtype=torch.float)\n",
        "\n",
        "#     # Create a fully connected graph for simplicity\n",
        "#     num_nodes = x.size(0)\n",
        "#     edge_index = torch.combinations(torch.arange(num_nodes), r=2).T\n",
        "#     edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)  # Make edges bidirectional\n",
        "\n",
        "#     return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "def create_knn_unified_graph(data, sensor_cols, k=5):\n",
        "    # Features: RSSI values\n",
        "    x = torch.tensor(data[sensor_cols].values, dtype=torch.float)\n",
        "    # Labels: Coordinates (Pos_x, Pos_y)\n",
        "    y = torch.tensor(data[['Pos_x', 'Pos_y', 'Floor_ID']].values, dtype=torch.float)\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    from sklearn.metrics.pairwise import euclidean_distances\n",
        "    distances = euclidean_distances(data[sensor_cols])\n",
        "    edge_index = []\n",
        "\n",
        "    # Construct edges using KNN\n",
        "    for i in range(distances.shape[0]):\n",
        "        neighbors = np.argsort(distances[i])[:k]  # Get k nearest neighbors\n",
        "        for j in neighbors:\n",
        "            edge_index.append([i, j])\n",
        "\n",
        "    # Convert edge list to PyTorch Geometric format\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).T\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "\n",
        "# Create graph for new dataset\n",
        "# new_global_graph = create_unified_graph(df_test, sensor_cols)\n",
        "new_global_graph = create_knn_unified_graph(df_test, sensor_cols, k=5)\n",
        "\n",
        "# Evaluate the global model on the new graph\n",
        "print(\"Evaluating on the new dataset...\")\n",
        "evaluate_model(model, new_global_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPZenRmKoVCg",
        "outputId": "d7214033-da66-41f4-ab98-f6bad0100bdd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on the new dataset...\n",
            "Evaluation Results X:\n",
            "MAE: 16.2252\n",
            "RMSE: 19.6578\n",
            "MAPE: 33.4324%\n",
            "Evaluation Results Y:\n",
            "MAE: 4.0980\n",
            "RMSE: 5.4321\n",
            "MAPE: 35.3372%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_final(graph_data, model, epochs=200, lr=0.01):\n",
        "    graph_data = graph_data.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(graph_data.x, graph_data.edge_index)\n",
        "        loss = criterion(pred, graph_data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "    return model\n",
        "\n",
        "final_train_graph = create_knn_graphs_by_floor(df_train, sensor_cols, k=5)\n",
        "test_graph = create_knn_graphs_by_floor(df_test, sensor_cols, k=5)\n",
        "curr_floor = 0\n",
        "for floor_id in sorted(final_train_graph.keys()):\n",
        "    print(f\"Training/Fine-tuning on Floor {floor_id}...\")\n",
        "    model_final = train_model_final(final_train_graph[floor_id], model)\n",
        "    curr_floor = floor_id\n",
        "    print(\"Evaluating model on floor \" + str(floor_id) + \"...\")\n",
        "    evaluate_model(model, test_graph[floor_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szq4trA86EdV",
        "outputId": "02ce0b77-362b-4866-c57e-713b110a3429"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Fine-tuning on Floor -3...\n",
            "Epoch 20, Loss: 8.517341613769531\n",
            "Epoch 40, Loss: 7.207226276397705\n",
            "Epoch 60, Loss: 5.008084774017334\n",
            "Epoch 80, Loss: 4.564362525939941\n",
            "Epoch 100, Loss: 4.387905120849609\n",
            "Epoch 120, Loss: 4.278393268585205\n",
            "Epoch 140, Loss: 4.198227405548096\n",
            "Epoch 160, Loss: 4.134030818939209\n",
            "Epoch 180, Loss: 4.080323696136475\n",
            "Epoch 200, Loss: 4.034214973449707\n",
            "Evaluating model on floor -3...\n",
            "Evaluation Results X:\n",
            "MAE: 9.7613\n",
            "RMSE: 11.2880\n",
            "MAPE: 12.4914%\n",
            "Evaluation Results Y:\n",
            "MAE: 2.1046\n",
            "RMSE: 2.5900\n",
            "MAPE: 10.2582%\n",
            "Training/Fine-tuning on Floor -2...\n",
            "Epoch 20, Loss: 30.67874526977539\n",
            "Epoch 40, Loss: 11.752347946166992\n",
            "Epoch 60, Loss: 8.74910831451416\n",
            "Epoch 80, Loss: 8.150623321533203\n",
            "Epoch 100, Loss: 7.69141149520874\n",
            "Epoch 120, Loss: 7.40309476852417\n",
            "Epoch 140, Loss: 7.16849946975708\n",
            "Epoch 160, Loss: 6.972365379333496\n",
            "Epoch 180, Loss: 6.807369709014893\n",
            "Epoch 200, Loss: 6.667547702789307\n",
            "Evaluating model on floor -2...\n",
            "Evaluation Results X:\n",
            "MAE: 12.3591\n",
            "RMSE: 14.6304\n",
            "MAPE: 15.7667%\n",
            "Evaluation Results Y:\n",
            "MAE: 2.8392\n",
            "RMSE: 3.4734\n",
            "MAPE: 14.3425%\n",
            "Training/Fine-tuning on Floor -1...\n",
            "Epoch 20, Loss: 76.8125\n",
            "Epoch 40, Loss: 36.18605422973633\n",
            "Epoch 60, Loss: 29.028465270996094\n",
            "Epoch 80, Loss: 25.873994827270508\n",
            "Epoch 100, Loss: 24.888954162597656\n",
            "Epoch 120, Loss: 24.248374938964844\n",
            "Epoch 140, Loss: 23.747018814086914\n",
            "Epoch 160, Loss: 23.299345016479492\n",
            "Epoch 180, Loss: 22.895584106445312\n",
            "Epoch 200, Loss: 22.52708625793457\n",
            "Evaluating model on floor -1...\n",
            "Evaluation Results X:\n",
            "MAE: 6.4207\n",
            "RMSE: 7.9070\n",
            "MAPE: 10.2276%\n",
            "Evaluation Results Y:\n",
            "MAE: 5.8272\n",
            "RMSE: 6.2162\n",
            "MAPE: 73.9004%\n",
            "Training/Fine-tuning on Floor 0...\n",
            "Epoch 20, Loss: 107.40062713623047\n",
            "Epoch 40, Loss: 63.01076889038086\n",
            "Epoch 60, Loss: 44.44463348388672\n",
            "Epoch 80, Loss: 38.90162658691406\n",
            "Epoch 100, Loss: 36.41474151611328\n",
            "Epoch 120, Loss: 34.76856994628906\n",
            "Epoch 140, Loss: 33.487430572509766\n",
            "Epoch 160, Loss: 32.430912017822266\n",
            "Epoch 180, Loss: 31.527246475219727\n",
            "Epoch 200, Loss: 30.733423233032227\n",
            "Evaluating model on floor 0...\n",
            "Evaluation Results X:\n",
            "MAE: 13.4396\n",
            "RMSE: 14.1605\n",
            "MAPE: 20.3475%\n",
            "Evaluation Results Y:\n",
            "MAE: 4.0499\n",
            "RMSE: 4.8520\n",
            "MAPE: 50.7460%\n",
            "Training/Fine-tuning on Floor 1...\n",
            "Epoch 20, Loss: 41.72023391723633\n",
            "Epoch 40, Loss: 29.060781478881836\n",
            "Epoch 60, Loss: 24.948129653930664\n",
            "Epoch 80, Loss: 22.521650314331055\n",
            "Epoch 100, Loss: 20.87364387512207\n",
            "Epoch 120, Loss: 19.779613494873047\n",
            "Epoch 140, Loss: 18.97281837463379\n",
            "Epoch 160, Loss: 18.373680114746094\n",
            "Epoch 180, Loss: 17.88507080078125\n",
            "Epoch 200, Loss: 17.464630126953125\n",
            "Evaluating model on floor 1...\n",
            "Evaluation Results X:\n",
            "MAE: 7.3169\n",
            "RMSE: 9.1093\n",
            "MAPE: 17.1742%\n",
            "Evaluation Results Y:\n",
            "MAE: 2.4674\n",
            "RMSE: 2.8558\n",
            "MAPE: 16.9376%\n",
            "Training/Fine-tuning on Floor 2...\n",
            "Epoch 20, Loss: 74.23615264892578\n",
            "Epoch 40, Loss: 36.03948211669922\n",
            "Epoch 60, Loss: 28.86849594116211\n",
            "Epoch 80, Loss: 25.53535270690918\n",
            "Epoch 100, Loss: 23.57456398010254\n",
            "Epoch 120, Loss: 22.205434799194336\n",
            "Epoch 140, Loss: 21.210430145263672\n",
            "Epoch 160, Loss: 20.446535110473633\n",
            "Epoch 180, Loss: 19.85004234313965\n",
            "Epoch 200, Loss: 19.378952026367188\n",
            "Evaluating model on floor 2...\n",
            "Evaluation Results X:\n",
            "MAE: 7.9837\n",
            "RMSE: 8.8284\n",
            "MAPE: 9.5854%\n",
            "Evaluation Results Y:\n",
            "MAE: 5.1432\n",
            "RMSE: 5.5405\n",
            "MAPE: 26.9265%\n",
            "Training/Fine-tuning on Floor 3...\n",
            "Epoch 20, Loss: 40.32826232910156\n",
            "Epoch 40, Loss: 30.49354362487793\n",
            "Epoch 60, Loss: 24.447551727294922\n",
            "Epoch 80, Loss: 21.46119499206543\n",
            "Epoch 100, Loss: 19.729299545288086\n",
            "Epoch 120, Loss: 18.34210777282715\n",
            "Epoch 140, Loss: 17.211009979248047\n",
            "Epoch 160, Loss: 16.3068904876709\n",
            "Epoch 180, Loss: 15.590846061706543\n",
            "Epoch 200, Loss: 15.008633613586426\n",
            "Evaluating model on floor 3...\n",
            "Evaluation Results X:\n",
            "MAE: 12.6267\n",
            "RMSE: 14.1537\n",
            "MAPE: 31.1674%\n",
            "Evaluation Results Y:\n",
            "MAE: 1.4959\n",
            "RMSE: 1.5955\n",
            "MAPE: 8.2235%\n",
            "Training/Fine-tuning on Floor 4...\n",
            "Epoch 20, Loss: 70.79594421386719\n",
            "Epoch 40, Loss: 40.1981201171875\n",
            "Epoch 60, Loss: 33.48955154418945\n",
            "Epoch 80, Loss: 30.095930099487305\n",
            "Epoch 100, Loss: 28.163043975830078\n",
            "Epoch 120, Loss: 26.133026123046875\n",
            "Epoch 140, Loss: 24.921506881713867\n",
            "Epoch 160, Loss: 23.91609764099121\n",
            "Epoch 180, Loss: 23.059375762939453\n",
            "Epoch 200, Loss: 22.31248664855957\n",
            "Evaluating model on floor 4...\n",
            "Evaluation Results X:\n",
            "MAE: 10.9698\n",
            "RMSE: 12.7538\n",
            "MAPE: 22.0391%\n",
            "Evaluation Results Y:\n",
            "MAE: 2.2667\n",
            "RMSE: 3.2534\n",
            "MAPE: 9.7728%\n",
            "Training/Fine-tuning on Floor 5...\n",
            "Epoch 20, Loss: 51.417701721191406\n",
            "Epoch 40, Loss: 33.35501480102539\n",
            "Epoch 60, Loss: 24.014192581176758\n",
            "Epoch 80, Loss: 20.455703735351562\n",
            "Epoch 100, Loss: 18.652637481689453\n",
            "Epoch 120, Loss: 17.182748794555664\n",
            "Epoch 140, Loss: 16.12050437927246\n",
            "Epoch 160, Loss: 15.298782348632812\n",
            "Epoch 180, Loss: 14.691590309143066\n",
            "Epoch 200, Loss: 14.18990707397461\n",
            "Evaluating model on floor 5...\n",
            "Evaluation Results X:\n",
            "MAE: 8.1148\n",
            "RMSE: 9.5908\n",
            "MAPE: 17.6986%\n",
            "Evaluation Results Y:\n",
            "MAE: 3.6099\n",
            "RMSE: 5.5793\n",
            "MAPE: 35.3484%\n",
            "Training/Fine-tuning on Floor 6...\n",
            "Epoch 20, Loss: 59.684749603271484\n",
            "Epoch 40, Loss: 44.930564880371094\n",
            "Epoch 60, Loss: 38.605262756347656\n",
            "Epoch 80, Loss: 34.32366180419922\n",
            "Epoch 100, Loss: 31.81947898864746\n",
            "Epoch 120, Loss: 29.830402374267578\n",
            "Epoch 140, Loss: 28.302146911621094\n",
            "Epoch 160, Loss: 26.878183364868164\n",
            "Epoch 180, Loss: 25.762554168701172\n",
            "Epoch 200, Loss: 24.8802433013916\n",
            "Evaluating model on floor 6...\n",
            "Evaluation Results X:\n",
            "MAE: 6.4632\n",
            "RMSE: 8.1548\n",
            "MAPE: 14.1752%\n",
            "Evaluation Results Y:\n",
            "MAE: 5.1270\n",
            "RMSE: 5.5257\n",
            "MAPE: 25.8388%\n",
            "Training/Fine-tuning on Floor 7...\n",
            "Epoch 20, Loss: 70.61619567871094\n",
            "Epoch 40, Loss: 62.740413665771484\n",
            "Epoch 60, Loss: 54.23141098022461\n",
            "Epoch 80, Loss: 49.710533142089844\n",
            "Epoch 100, Loss: 46.61898422241211\n",
            "Epoch 120, Loss: 44.13690185546875\n",
            "Epoch 140, Loss: 41.99056625366211\n",
            "Epoch 160, Loss: 39.99513244628906\n",
            "Epoch 180, Loss: 37.652164459228516\n",
            "Epoch 200, Loss: 35.70793533325195\n",
            "Evaluating model on floor 7...\n",
            "Evaluation Results X:\n",
            "MAE: 12.5909\n",
            "RMSE: 15.2180\n",
            "MAPE: 28.4602%\n",
            "Evaluation Results Y:\n",
            "MAE: 3.4804\n",
            "RMSE: 4.2015\n",
            "MAPE: 18.3292%\n",
            "Training/Fine-tuning on Floor 8...\n",
            "Epoch 20, Loss: 80.13874053955078\n",
            "Epoch 40, Loss: 79.77128601074219\n",
            "Epoch 60, Loss: 68.22154998779297\n",
            "Epoch 80, Loss: 63.51652145385742\n",
            "Epoch 100, Loss: 60.49281692504883\n",
            "Epoch 120, Loss: 57.49455261230469\n",
            "Epoch 140, Loss: 55.02458953857422\n",
            "Epoch 160, Loss: 53.45663070678711\n",
            "Epoch 180, Loss: 50.66521453857422\n",
            "Epoch 200, Loss: 48.83688735961914\n",
            "Evaluating model on floor 8...\n",
            "Evaluation Results X:\n",
            "MAE: 10.9979\n",
            "RMSE: 14.4982\n",
            "MAPE: 20.0101%\n",
            "Evaluation Results Y:\n",
            "MAE: 5.9724\n",
            "RMSE: 6.7689\n",
            "MAPE: 59.1872%\n",
            "Training/Fine-tuning on Floor 9...\n",
            "Epoch 20, Loss: 86.06624603271484\n",
            "Epoch 40, Loss: 47.2493782043457\n",
            "Epoch 60, Loss: 35.40034484863281\n",
            "Epoch 80, Loss: 31.878902435302734\n",
            "Epoch 100, Loss: 29.594804763793945\n",
            "Epoch 120, Loss: 27.97177505493164\n",
            "Epoch 140, Loss: 26.72808074951172\n",
            "Epoch 160, Loss: 25.73809051513672\n",
            "Epoch 180, Loss: 24.92599868774414\n",
            "Epoch 200, Loss: 24.243417739868164\n",
            "Evaluating model on floor 9...\n",
            "Evaluation Results X:\n",
            "MAE: 15.0079\n",
            "RMSE: 16.2130\n",
            "MAPE: 22.0313%\n",
            "Evaluation Results Y:\n",
            "MAE: 5.4662\n",
            "RMSE: 6.0247\n",
            "MAPE: 31.7214%\n",
            "Training/Fine-tuning on Floor 10...\n",
            "Epoch 20, Loss: 52.88015365600586\n",
            "Epoch 40, Loss: 35.762115478515625\n",
            "Epoch 60, Loss: 28.57419776916504\n",
            "Epoch 80, Loss: 25.505212783813477\n",
            "Epoch 100, Loss: 23.64436149597168\n",
            "Epoch 120, Loss: 22.220266342163086\n",
            "Epoch 140, Loss: 21.069374084472656\n",
            "Epoch 160, Loss: 20.108455657958984\n",
            "Epoch 180, Loss: 19.286439895629883\n",
            "Epoch 200, Loss: 18.577125549316406\n",
            "Evaluating model on floor 10...\n",
            "Evaluation Results X:\n",
            "MAE: 4.0552\n",
            "RMSE: 4.5896\n",
            "MAPE: 9.9525%\n",
            "Evaluation Results Y:\n",
            "MAE: 3.2452\n",
            "RMSE: 3.7724\n",
            "MAPE: 16.2210%\n",
            "Training/Fine-tuning on Floor 11...\n",
            "Epoch 20, Loss: 48.37546157836914\n",
            "Epoch 40, Loss: 32.0206298828125\n",
            "Epoch 60, Loss: 25.5623836517334\n",
            "Epoch 80, Loss: 22.74030303955078\n",
            "Epoch 100, Loss: 20.738874435424805\n",
            "Epoch 120, Loss: 19.288223266601562\n",
            "Epoch 140, Loss: 18.20018768310547\n",
            "Epoch 160, Loss: 17.36568260192871\n",
            "Epoch 180, Loss: 16.711406707763672\n",
            "Epoch 200, Loss: 16.1853084564209\n",
            "Evaluating model on floor 11...\n",
            "Evaluation Results X:\n",
            "MAE: 32.7323\n",
            "RMSE: 42.5418\n",
            "MAPE: 50.6922%\n",
            "Evaluation Results Y:\n",
            "MAE: 4.0498\n",
            "RMSE: 4.6484\n",
            "MAPE: 23.0896%\n",
            "Training/Fine-tuning on Floor 12...\n",
            "Epoch 20, Loss: 107.60472869873047\n",
            "Epoch 40, Loss: 33.091548919677734\n",
            "Epoch 60, Loss: 27.872352600097656\n",
            "Epoch 80, Loss: 24.90483283996582\n",
            "Epoch 100, Loss: 23.197032928466797\n",
            "Epoch 120, Loss: 21.928926467895508\n",
            "Epoch 140, Loss: 20.941404342651367\n",
            "Epoch 160, Loss: 20.149377822875977\n",
            "Epoch 180, Loss: 19.506000518798828\n",
            "Epoch 200, Loss: 18.97648811340332\n",
            "Evaluating model on floor 12...\n",
            "Evaluation Results X:\n",
            "MAE: 7.4026\n",
            "RMSE: 8.3412\n",
            "MAPE: 15.2367%\n",
            "Evaluation Results Y:\n",
            "MAE: 2.7870\n",
            "RMSE: 3.6167\n",
            "MAPE: 12.9724%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch_geometric.data import DataLoader\n",
        "\n",
        "# # Loop through each floor's dataset\n",
        "# models_per_floor = {}\n",
        "# for floor_id, floor_data in floor_datasets.items():\n",
        "#     # Preprocess data for the floor (convert to graph)\n",
        "#     graph_data = preprocess_graph(floor_data)  # Define preprocessing logic\n",
        "\n",
        "#     # Train a GraphSAGE model\n",
        "#     model = GraphSAGEModel(input_dim=graph_data.num_node_features, hidden_dim=64, output_dim=2)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         coords_pred, floor_pred = model(graph_data)\n",
        "#         loss = nn.MSELoss()(coords_pred, graph_data.y[:, :2])  # Only use (Pos_x, Pos_y)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Save trained model for the floor\n",
        "#     models_per_floor[floor_id] = model"
      ],
      "metadata": {
        "id": "kZqfj1J1lNef"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}